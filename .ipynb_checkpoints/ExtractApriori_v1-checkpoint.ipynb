{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba0cbff4",
   "metadata": {},
   "source": [
    "# Spatial and temporal resampling of gridded ECMWF data onto a satellite's swath\n",
    "\n",
    "For the retrieval of physical parameters in the atmosphere via Optimal Estimation, we need apriori knowlegdge of the quantity we want to retrieve; this apriori knowledge can be obtained from a forecast model. \n",
    "\n",
    "ECMWF's forecasts are available in different spatial and temporal resolutions, in particular in this notebook we work with a *0.25/0.25* regular *lon/lat* grid in **space** and 16 steps for 2 [analysis](https://www.ecmwf.int/en/research/data-assimilation) times per day, time/step combination gives **temporal** coverage of data every hour: \n",
    "\n",
    "- *time* is the reference time (time at which the analysis is performed and observations are used to update the model).\n",
    "- *step* is related to the time steps that describe the temporal evolution of the forecast model.\n",
    "\n",
    "Once the ECMWF's data is available with a given spatial/temporal resolution, we focus on the satellite observations, to be specific the locations (i.e. longitude and latitude of each observation); because the satellite does not care much about grids, we rely on it's swath definition: how the instrument *samples* in space and time.\n",
    "\n",
    "Different instruments (and data providers) might have different versions of how to describe the swath (however the concept of swath is happily enough, unique); in our case we use CMSAF [data](https://wui.cmsaf.eu/safira/action/viewDoiDetails?acronym=FCDR_MWI_V003) (i.e. Brightness Temperatures). \n",
    "Each specific sample (observation) is made at a specific time and location (*time*, *lon*, *lat*), so our goal is: to interpolate the ECMWF's variable (which is defined on the regular *lon/lat* grid) onto the specific time/locations in our satellite's swath. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29ffd72",
   "metadata": {},
   "source": [
    "In this notebook we use: \n",
    "- [Pyresample](https://pyresample.readthedocs.io/en/latest/)'s functionality to efficiently resample data from a regular grid onto a swath.\n",
    "- [xarray](https://docs.xarray.dev/en/stable/)'s functionality to handle high dimensional datasets and to perform the time interpolation. \n",
    "\n",
    "First we will import the needed libraries and packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c342a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import xarray as xr\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "\n",
    "import cartopy\n",
    "import cartopy.crs as ccrs\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "\n",
    "import pyproj\n",
    "import pyresample\n",
    "from pyresample import create_area_def, load_area, data_reduce, utils, AreaDefinition\n",
    "from pyresample.geometry import SwathDefinition, GridDefinition\n",
    "from pyresample.kd_tree import resample_nearest, resample_gauss \n",
    "from pyresample.bilinear import XArrayBilinearResampler, NumpyBilinearResampler #\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b0ccb7",
   "metadata": {},
   "source": [
    "I might use *Dask*'s functionality later on; when working with large datasets, *xarray* offers *Dask*'s capabilities to chunk, out-of-memory computation and parallel processing.\n",
    "\n",
    "Here we just create a computing cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebbc0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"MALLOC_TRIM_THRESHOLD_\"] = \"0\"#\"65536\"\n",
    "\n",
    "from dask.distributed import Client, progress, LocalCluster\n",
    "\n",
    "cluster = LocalCluster()\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3404236",
   "metadata": {},
   "source": [
    "We define some directories for easy access to the datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde466cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Satellite data:\n",
    "#dataSatDir = '/home/mario/Data/CMSAF/ssims/F16/'\n",
    "dataSatDir = '/home/mario/Data/CMSAF/ssims/F16/ORD47662/'\n",
    "#dataSatDir = '/nobackup/users/echeverr/data/cmsaf/ssmis/F16/'\n",
    "#fileSatID = 'BTRin20140909000000324SSF1601GL.nc'\n",
    "\n",
    "# ECMWF data:\n",
    "#dataECMWFDir ='/home/mario/Data/Covariance_means/MARS_api_data/datasetsApriori/'\n",
    "#dataECMWFDir = '/nobackup/users/echeverr/data/ECMWF_era5/MARS_api_data/datasetsApriori/'\n",
    "dataECMWFDir ='/home/mario/Data/Covariance_means/MARS_api_data/datasetsAprioriRegGrid/'\n",
    "\n",
    "# REMOVE THE FOLLOWING ONCE YOU GET ECMWF'S DATASETS IN REGULAR GRID:\n",
    "#Test only:\n",
    "\n",
    "auxDataECMWFDir = '/home/mario/Data/Covariance_means/MARS_api_data/ERA5_data/datasets/'\n",
    "\n",
    "\n",
    "sys.path.append('support') # where supporting_routines_m live\n",
    "\n",
    "import support_routines "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d38d83",
   "metadata": {},
   "source": [
    "# Data preparation\n",
    "\n",
    "We have two sources of data that we care about in this notebook: ECMWF's data and satellite's swath definition (we do not use explicitely the observations per se, rather we use the location of the observations, the points where we want to resample and interpolate our ECMWF data)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da951760",
   "metadata": {},
   "source": [
    "First we load ECMWF's datasets using xarray; notice we open *profile* like datasets (i.e. with variables defined in pressure or model levels) and *surface* like datasets (i.e. with variables defined on the surface, e.g. 10m wind speed or 2m temperature).\n",
    "\n",
    "I then merge them into a single working dataset; shared dimensions and coordinates are automatically handled by xarray:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1b888b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "profile_info = xr.open_mfdataset(dataECMWFDir+'profiles*.grib', \n",
    "                                 engine=\"cfgrib\") #, chunks={'time': 50, 'latitude': 50, 'longitude': 200})\n",
    "surface_info = xr.open_mfdataset(dataECMWFDir+'surface*.grib', \n",
    "                                 engine=\"cfgrib\") #, chunks={'time': 50,'latitude': 50, 'longitude': 200})\n",
    "\n",
    "\n",
    "work_ds = profile_info.merge(surface_info).copy()\n",
    "\n",
    "work_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e5453f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block is only auxiliary: I used \"cdo\" to convert \n",
    "# a reduced Gaussian grid dataset (N320) into a \n",
    "# regular 0.25x0.25 deg**2 grid (ECMWF MARS was not\n",
    "# available at the time of this test, so I could not \n",
    "# download the dataset in the regular grid).\n",
    "# \"cdo\" uses a bilinear interpolation (cdo documentation); \n",
    "# but the resulting grid (lon, lat) has slight \n",
    "# differences respect to the ECMWF regular grid.\n",
    "\n",
    "# In this block I just take the (lon,lat) from another \n",
    "# 0.25x0.25 deg**2 ECMWF dataset and replace my cdo \n",
    "# regular grid with it for consistency.\n",
    "\n",
    "aux_info = xr.open_mfdataset(auxDataECMWFDir+'surface*.grib', \n",
    "                                 engine=\"cfgrib\")\n",
    "work_ds['latitude'] = aux_info['latitude'][::-1] # The order in cdo is different\n",
    "work_ds['longitude'] = aux_info['longitude']\n",
    "work_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7b9920",
   "metadata": {},
   "source": [
    "Having forecasts separated in *time*/*step* is not useful for our endeavour; we would like to have a single time reference to refer to.\n",
    "Also we want 24 hours coverage for all the period of interest, therefore we will stack each analysis time (there are 2 analysis per day in the operational forecasts, 00:00 and 12:00) with it's 12 steps.\n",
    "\n",
    "Because our end goal will be to use ECMWF's forecasts as apriori information in the Optimal Estimation (where we retrieve physical variables using observations), we want to avoid using data next to the analysis time (where observations are being [assimilated](https://www.ecmwf.int/en/research/data-assimilation)) in order to avoid undesired correlations between the apriori and the observations. It is standard to take a window of 3 hours after the analysis for the apriori data (you can notice this in the previous dataset, where the coordinate *step* starts at 03:00).  \n",
    "\n",
    "We use stack to have a single reference time at the end, we call this new reference time **time2** for simplicity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0070a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ECMWF_ds = work_ds.isel(step=slice(0,12)).stack(time2=(\"time\",\"step\"))\n",
    "ECMWF_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9979ba56",
   "metadata": {},
   "source": [
    "After stacking time/step the new multi-index variable **time2** is not very useful as a time reference, instead we create the new time dimension as the sum of the analysis time and the step (so time + step).\n",
    "\n",
    "It is useful to point out that *longitude* values for using *pyresample* *must* be refered to a *\\[-180, 180\\]* range (instead of *\\[0, 360\\]* )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0500b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ECMWF_ds['time2'] = (work_ds.isel(step=slice(0,12)).time + \n",
    " work_ds.isel(step=slice(0,12)).step).stack(time2=(\"time\",\"step\"))\n",
    "#ECMWF_ds['longitude'].values = ECMWF_ds.longitude.values - 180.0    # Reset lon to [-180,180]\n",
    "ECMWF_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c331711c",
   "metadata": {},
   "source": [
    "Our ECMWF's dataset seems to be ready to use; we now focus on our satellite observations.\n",
    "\n",
    "In this notebook we use CMSAF's [data](https://wui.cmsaf.eu/safira/action/viewDoiDetails?acronym=FCDR_MWI_V003) (Temperature Brightness).\n",
    "\n",
    "CMSAF's data is structured in logical groups, where each group coincides with a group of channels that share the same antenna of the instrument; this logical separation is very useful because different antennas will (likely) have effectively different footprints and sampling on the ground. \n",
    "\n",
    "We first open the datasets (7 days of observations, overlaping as much as possible in time with our ECMWF data); the open method in xarray (*open_mfdataset*) will open only the highest level in the datasets, this is useful to grasp the contents of the dataset (*channels*, *time*, *swath*, etc.). Notice that *open_mfdataset* at a difference with the more basic *open_dataset* will load the datasets in a *lazy* way using **Dask** [under the hood](https://docs.xarray.dev/en/stable/user-guide/dask.html) to avoid actually loading the data on memory:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730c88aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open satellite dataset at highest level (just to get the channels information):\n",
    "\n",
    "#ds = xr.open_dataset(dataSatDir+fileID)\n",
    "ds = xr.open_mfdataset(dataSatDir+'*.nc')\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66b8dda",
   "metadata": {},
   "source": [
    "Logical groups in NetCDF files (.nc) can be accessed directly with *xarray* if you know the name of the group (this can be easily accessed via the NetCDF4 library if not given by the data provider).\n",
    "\n",
    "For our particular application (10m wind speed retrieval via Optimal Estimation) we want to access the groups *scene_env1* and *scene_env2* (containing channels 19 and 37 GHz horizontal/vertical polarizations):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fee0a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open specific scenes containing the satellite observations:\n",
    "\n",
    "scenes_list = ['scene_env1', 'scene_env2']\n",
    "scene_BT = []\n",
    "\n",
    "for scene in scenes_list:        \n",
    "    scene_BT.append(xr.open_mfdataset(\n",
    "        dataSatDir+'*.nc', combine = 'nested', \n",
    "        concat_dim='time', group = scene)) \n",
    "\n",
    "#for scene in scenes_list:\n",
    "    #scene_BT.append(xr.open_dataset(dataSatDir+fileID, group = scene))\n",
    "    #scene_BT.append(xr.open_mfdataset(dataSatDir+'*.nc', group = scene))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb810d9",
   "metadata": {},
   "source": [
    "Lets check *scene_env1*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e28b5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_BT[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eab5eef",
   "metadata": {},
   "source": [
    "We can now  simply concatenate scenes *scene_env1* and *scene_env2*, given because they share the same swath definition (*lon*/*lat* values for each *time*/*scene_across_track* combination, you can check this in CMSAF's product [documentation](https://www.cmsaf.eu/SharedDocs/Literatur/document/2016/saf_cm_dwd_pum_fcdr_ssmis_1_4_pdf.pdf?__blob=publicationFile))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dad8276",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_BT = xr.concat(scene_BT, dim = 'scene_channel') #.drop_vars([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a85f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_BT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45581dc",
   "metadata": {},
   "source": [
    "After concatenating through a dimension (in our case through *scene_channel*) *xarray* fill's in any \"missing\" information; in our case we se that *xarray* added the dimension *scene_channel* to variables that do not really depend on it.\n",
    "\n",
    "Here we simply correct this by selecting the first element in this dimension for all the variables that actually do not depend on it; we also notice that the attributes (e.b. comment: channels h19, v19, etc.) are not complete after the concatenation, **we will not focus on this** in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db80522a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_BT['lat'] = ds_BT.lat[0,:,:]\n",
    "ds_BT['lon'] = ds_BT.lon[0,:,:]\n",
    "ds_BT['eia'] = ds_BT.eia[0,:,:]\n",
    "ds_BT['sft'] = ds_BT.sft[0,:,:]\n",
    "ds_BT['qc_fov'] = ds_BT.qc_fov[0,:,:]\n",
    "ds_BT['laz'] = ds_BT.laz[0,:,:]\n",
    "\n",
    "# And visualize again:\n",
    "ds_BT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32a2111",
   "metadata": {},
   "source": [
    "Finally we want to keep track of the frequency of the channels that we will use, so we select the channels that we will use (and only over the ocean in this case, where sft==0). We also copy the *central_freq* and *polarization* variables and use the values that we already had in our higher level dataset *ds*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3dee53",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_aux = ds_BT.assign_coords(time=ds.time).sel(\n",
    "    scene_channel=[11,12,14,15]).where(ds_BT.sft==0)\n",
    "\n",
    "ds_aux['central_freq'] = ds['central_freq'][0,0,ds_aux['scene_channel']]\n",
    "ds_aux['polarization'] = ds['polarization'][0,0,ds_aux['scene_channel']]\n",
    "\n",
    "# Create working satellite dataset (setting 'scene_channel' as last dimension):\n",
    "\n",
    "SAT_ds = ds_aux.transpose(...,'scene_channel') #.drop_dims(drop_dims = ['date','channel'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce27c8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAT_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16852296",
   "metadata": {},
   "source": [
    "# Spatial resampling and temporal interpolation of ECMWF's data onto our satellite's swath\n",
    "\n",
    "We have arrived to the main part of this notebook, our datasets are ready to be used. \n",
    "\n",
    "Our time reference will be the satellite observations time (after all, we are getting apriori data to eventually process our satellite data).\n",
    "\n",
    "A summary of the chain of processing is:\n",
    "- User defines a time period of observations (lets call it a **batch**) for which resample/interpolation of ECMWF's data is needed.\n",
    "- We will automatically select from ECMWF's available data the required time slices, including 1, 2 or as many as you want time slices before the initial satellite time and 1,2 or as many as you want time slices after the end time. This simply to have enough points for the interpolation in time (including beginning and end of observations).\n",
    "- We use pyresample to resample all the selected time slices from ECMWF's data to the swath locations.\n",
    "- Finally we interpolate the resampled data in time for each location.\n",
    "\n",
    "So first, the user inputs the desired period and *xarray* helps us indexing the right (at least the nearest) period that corresponds to the **batch** the user asked for:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730eb6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User defined desired period of time to analyze:\n",
    "initSat_time = np.datetime64('2014-10-02T00:00:00.000') \n",
    "endSat_time = np.datetime64('2014-10-02T00:45:59.000')\n",
    "\n",
    "# Find best match (e.g. nearest) for the times present in the dataset:\n",
    "init_time = SAT_ds.time.sel(time=initSat_time, method = \"nearest\")\n",
    "end_time = SAT_ds.time.sel(time=endSat_time, method = \"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de85e6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "work_SAT_ds = SAT_ds.sel(time=slice(init_time,end_time))\n",
    "                             \n",
    "work_SAT_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82836778",
   "metadata": {},
   "source": [
    "Second, we select enough points in time to cover amply the overlaping.\n",
    "Initial and final times of the overlap between the two datasets (satellite and ECMWF).\n",
    "We select as initial time the initial observation time \"minus\" x hours and as final time the final observation time \"plus\" x hours (x is 1, 2 or whatever is desired)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e55fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_h = np.timedelta64(1, 'h') # Useful delta to create overlap in time\n",
    "\n",
    "timeOverlapInit = ECMWF_ds.time2.sel(\n",
    "    time2 = work_SAT_ds.time.min() - delta_h, method = \"nearest\")\n",
    "\n",
    "timeOverlapEnd = ECMWF_ds.time2.sel(\n",
    "    time2=work_SAT_ds.time.max() + delta_h, method = \"nearest\")\n",
    "\n",
    "work_ECMWF_ds = ECMWF_ds.sel(time2 = \n",
    "                             slice(timeOverlapInit,timeOverlapEnd)\n",
    "                            )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d2552e",
   "metadata": {},
   "source": [
    "We reorder the dimensions with *time2* as last dimension; this because we want to exploit pyresample's ability to resample multiple \"channels\" at the same time, as long as the \"channels\" (or time instants in this setting) are located in the last dimension.\n",
    "\n",
    "Just to be clear, we do not have channels in ECMWF's data, but we do have time instants or slices, and *pyresample* offers very efficient implementations to resample 3D, tensor-shaped variables where the *lat*/*lon* variables are in the first 2 dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30af157c",
   "metadata": {},
   "outputs": [],
   "source": [
    "work_ECMWF_ds = work_ECMWF_ds.transpose(...,\n",
    "                                        'latitude','longitude','time2')\n",
    "work_ECMWF_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f17ea50",
   "metadata": {},
   "source": [
    "At this point I invite the reader to pause a moment to check the datasets at hand **work_SAT_ds** and **work_ECMWF_ds**; check the time coordinates (*time2* in the case of ECMWF's data, its just a name)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5634aaf7",
   "metadata": {},
   "source": [
    "Now we come to *pyresample* specifics; in order to resample, *pyresample* uses a [*geometry*](https://pyresample.readthedocs.io/en/latest/geo_def.html) definition, there are a couple of them available. Here we use the swath definition (for the satellite data) and the grid definition (for ECMWF's gridded data).\n",
    "\n",
    "The definitions are more or less self-explanatory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279eae06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define swath using PyResample's SwathDefinition (geometry def.): \n",
    "SAT_swath_def = SwathDefinition(lons = work_SAT_ds.lon.values, \n",
    "                                lats = work_SAT_ds.lat.values)\n",
    "\n",
    "# Define grid using PyResample's GridDefiniton (geometry def.):\n",
    "lon2d,lat2d = np.meshgrid(work_ECMWF_ds.longitude, \n",
    "                          work_ECMWF_ds.latitude)\n",
    "ECMWF_grid_def = GridDefinition(lons=lon2d, lats=lat2d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16625cd2",
   "metadata": {},
   "source": [
    "At this point we are ready to use the resampling methods (we use two to compare a bit, nearest neighbour and Gaussian weighted average).\n",
    "\n",
    "I compute and save immediately into an *xarray*'s dataarray within my satellite dataset; my resampled data is of course now defined in the satellite swath, so dimensions and coordinates are the same *exept* for a 3_{rd} dimension that comes from the time slices that I resampled.\n",
    "\n",
    "The 3^{rd} dimension contains the time slices that we will use to interpolate in time; for this reason I will simply rename this dimension as *time4interpolation*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fe04d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "work_SAT_ds['u10n_apriori_nn'] = xr.DataArray(\n",
    "                data   = resample_nearest(ECMWF_grid_def, work_ECMWF_ds.u10n.values, \\\n",
    "        SAT_swath_def, radius_of_influence=30000, fill_value=None),  # enter data here\n",
    "                dims   = ['time','scene_across_track','time4interpolation'],\n",
    "                coords = {'time': work_SAT_ds.time, \n",
    "                          'scene_across_track': work_SAT_ds.scene_across_track,\n",
    "                         'time4interpolation': work_ECMWF_ds.time2.values},\n",
    "                attrs  = {\n",
    "                    #'_FillValue': -999.9,\n",
    "                    'description': 'u10n from ECMWFs forecast resampled with\\\n",
    "                    PyResample (Nearest Neighbour resampler) to satellite swath',\n",
    "                    'units'     : 'm/s'\n",
    "                    }\n",
    "                ) #.chunk({\"time\": chunk_size_time,\n",
    "                  #       \"scene_across_track\": chunk_size_s_a_t})\n",
    "\n",
    "work_SAT_ds['u10n_apriori_gauss'] = xr.DataArray(\n",
    "                data   = resample_gauss(ECMWF_grid_def, work_ECMWF_ds.u10n.values, \\\n",
    "               SAT_swath_def, radius_of_influence=30000, neighbours=10,\\\n",
    "               sigmas=30000*np.ones(len(work_ECMWF_ds.time2.values)), fill_value=None),  # enter data here\n",
    "                dims   = ['time','scene_across_track','time4interpolation'],\n",
    "                coords = {'time': work_SAT_ds.time, \n",
    "                          'scene_across_track': work_SAT_ds.scene_across_track,\n",
    "                         'time4interpolation': work_ECMWF_ds.time2.values},\n",
    "                attrs  = {\n",
    "                    #'_FillValue': -999.9,\n",
    "                    'description': 'u10n from ECMWFs forecast resampled with\\\n",
    "                    PyResample (Nearest Neighbour resampler) to satellite swath',\n",
    "                    'units'     : 'm/s'\n",
    "                    }\n",
    "                ) #.chunk({\"time\": chunk_size_time,\n",
    "                  #       \"scene_across_track\": chunk_size_s_a_t})\n",
    "\n",
    "\n",
    "# BilinearResampler for xarray objects does not support either\n",
    "# SwathDefinition or GridDefinition at present! (26 Sept. 2022):\n",
    "\n",
    "#resampler = XArrayBilinearResampler(ECMWF_grid_def, SAT_swath_def, 50e3)\n",
    "#result = resampler.resample(work_ECMWF_ds.u10n[0,:,:])\n",
    "#result    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9661e36",
   "metadata": {},
   "source": [
    "We can now visualize our satellite dataset including the two resampling methods from *pyresample* that we used: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65dacda",
   "metadata": {},
   "outputs": [],
   "source": [
    "work_SAT_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfede7bc",
   "metadata": {},
   "source": [
    "# Time interpolation\n",
    "\n",
    "Once we have the resampled time slices from ECMWF's dataset we want to interpolate to a desired time instant; an important caveat here is that for simplicity and efficiency we interpolate to the midtime of the satellite data:\n",
    "\n",
    "So if our *init_time* is 00:00:00 and our *end_time* is 01:00:00 (so 1 hour), we will interpolate to 00:30:00. This is supported by the assumtion that our physical variables will not drastically change within the start and the end of the observation **batch**. \n",
    "\n",
    "We notice that once interpolated, the *time4interpolation* dimension does not longer exist; remember, you interpolated to a time instant, so in return you get that 'value' only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be6a6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time interpolation:\n",
    "# We want to interpolate to the middle of the observation batch:\n",
    "time_interp = init_time + (end_time-init_time)/2 \n",
    "\n",
    "# Select the nearest valid time i.e. a time instant\n",
    "# that exists in the observations:\n",
    "time_interp = work_SAT_ds.time.sel(time=time_interp, method = \"nearest\")\n",
    "time_interp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486b2e81",
   "metadata": {},
   "source": [
    "To interpolate through a single dimension (time in our case), *xarray* offers two main options:\n",
    "- Option 1: Use *linear interpolation* only (Or *Nearest Neighbours*)\n",
    "- Option 2: Use other available methods: *quadratic*, *cubic*, etc.\n",
    "\n",
    "However, there is a caveat:\n",
    "Other available methods (*quadratic*, *cubic*, *etc*.) do not support NaN, this means we need to get rid of NaN's before interpolating in *time4interpolation*. \n",
    "This can be tackled by 'pre-interpolating' around NaN's (via interpolate_na) and then droping the remaining NaN's. \n",
    "Check xarray.interp documentation for explanation on the use of: *interpolate_na()* and *dropna()*. Notice that although this 'pre-interpolation' is carried out in *time*, it is effectively an interpolation of our *spatial* samples (because of the definition of *swath* for our satellite observations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7c499f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolate using xarray's (scipy under the hood) capabilities:\n",
    "# xarray.interp documentation: \n",
    "# https://docs.xarray.dev/en/stable/user-guide/interpolation.html\n",
    "\n",
    "# Option 1: \n",
    "\n",
    "#work_SAT_ds['u10n_apriori_gauss_interp'] =\\\n",
    "#    work_SAT_ds.u10n_apriori_gauss.interp(\n",
    "#        time4interpolation = time_interp.values, method=\"linear\")\n",
    "\n",
    "# Option 2:\n",
    "\n",
    "work_SAT_ds['u10n_apriori_gauss_interp'] =\\\n",
    "    work_SAT_ds.u10n_apriori_gauss.interpolate_na(\"time\").dropna(\"time\")\\\n",
    "               .interp(time4interpolation=time_interp.values, method=\"cubic\")\n",
    "\n",
    "\n",
    "# Optional:\n",
    "# We also compute the Nearest Neigbours resampled version in order\n",
    "# to compare (in the end you have to choose one you want to use).\n",
    "\n",
    "work_SAT_ds['u10n_apriori_nn_interp'] =\\\n",
    "    work_SAT_ds.u10n_apriori_nn.interpolate_na(\"time\").dropna(\"time\")\\\n",
    "               .interp(time4interpolation=time_interp.values, method=\"cubic\")\n",
    "\n",
    "work_SAT_ds "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af1b834",
   "metadata": {},
   "source": [
    "At this point we have our ECMWF data resampled/interpolated in space/time and so the data is ready to use; lets visualize a few results.\n",
    "\n",
    "First, lets create an area of interest to plot, in this case I want to plot some of the computed variables on the entire globe:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f660eb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "area_def_world = load_area('areas.yaml', 'worldeqc30km')# 'worldeqc30km70') # for plots\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93654577",
   "metadata": {},
   "source": [
    "Lets visualize our observations (remember we selected a subset or **batch**):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ed6b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for channel in range(4):\n",
    "    reduced_lon_scene, reduced_lat_scene, reduced_data_scene =\\\n",
    "    support_routines.get_Sat_frame(work_SAT_ds, area_def_world, chan=channel, \n",
    "              var = 'tb', begin_t=None, end_t=None)    \n",
    "    support_routines.basicMapPlotScat1(reduced_lon_scene, reduced_lat_scene, reduced_data_scene,\n",
    "                 'scene_channel_'+str(channel), area_def_world, vmin=130, vmax=270,\n",
    "                                      proj=\"Orthographic\", \n",
    "                                       var = 'Channel: '+str(work_SAT_ds.central_freq[channel].values)+' '+\n",
    "                                       str(work_SAT_ds.central_freq[channel].units)+' '+\n",
    "                                      str(work_SAT_ds.polarization[channel].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbe46c9",
   "metadata": {},
   "source": [
    "For simplicity in the plotting I have created a single routine (**basicMapPlotScat1**) for plotting *swath* data (i.e. non regular grid data), we can visualize observations but also the resampled (and in the end, interpolated) variables; argument *chan* in the plotting routine will tell the routine wheter I want to visualize observations (*swath* + *channel*), resampled variables (*swath* + *time4interpolation*) or resampled **and** time interpolated variables (*swath* only).\n",
    "\n",
    "Now lets check the spatially resampled *u* component of the wind speed (i.e. not interpolated in time yet); for this I select one of the time slices that I resampled (*time4interpolation*):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2ce86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the resampled (Nearest neighb.) ECMWF data in the new 'grid' \n",
    "# (i.e. the satellite swath):\n",
    "\n",
    "reduced_lon_scene, reduced_lat_scene, reduced_data_scene =\\\n",
    "support_routines.get_Sat_frame(work_SAT_ds, area_def_world, chan=3, \n",
    "              var = 'u10n_apriori_nn', begin_t=None, end_t=None)\n",
    "\n",
    "support_routines.basicMapPlotScat1(reduced_lon_scene, reduced_lat_scene, reduced_data_scene,\n",
    "                 'resampledNN', area_def_world, vmin=-25, vmax=25, \n",
    "                                   proj=\"Orthographic\", var = \"u10n_apriori_nn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d98ce5b",
   "metadata": {},
   "source": [
    "It seems to produce a good (eye ball metric) resampled variable; lets check the ECMWF's gridded data on the same time slice: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2227de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot origin data (ECMWF on regular grid, to compare with the resampled one):\n",
    "\n",
    "fig = plt.figure()\n",
    "#ax = plt.axes(projection=ccrs.PlateCarree())  #\n",
    "ax = plt.axes(projection=ccrs.Orthographic(60,-15))\n",
    "ax.add_feature(cartopy.feature.LAND, zorder=0, edgecolor='black')\n",
    "ax.coastlines()\n",
    "ax.gridlines()\n",
    "work_ECMWF_ds.u10n[:,:,3].where(\n",
    "    work_ECMWF_ds.lsm[:,:,3]==0).plot(ax=ax,\n",
    "    transform=ccrs.PlateCarree(), cmap=\"viridis\",\n",
    "                                     vmin=-25, vmax=25)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "#plt.savefig('allWind_and_swath.png', bbox_inches='tight', dpi=300) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee949483",
   "metadata": {},
   "source": [
    "Now lets visualize the difference between the *Gaussian Nearest* and *Nearest Neighbours* methods in the **final resampled/interpolated (i.e. space/time)** variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5a920c",
   "metadata": {},
   "outputs": [],
   "source": [
    "work_SAT_ds['differenceNN_GN'] = np.abs(work_SAT_ds['u10n_apriori_gauss_interp']-\n",
    "                                        work_SAT_ds['u10n_apriori_nn_interp'])\n",
    "\n",
    "reduced_lon_scene, reduced_lat_scene, reduced_data_scene =\\\n",
    "support_routines.get_Sat_frame(work_SAT_ds, area_def_world, chan=-1, \n",
    "              var = 'differenceNN_GN', begin_t=None, end_t=None)\n",
    "\n",
    "support_routines.basicMapPlotScat1(reduced_lon_scene, reduced_lat_scene, reduced_data_scene,\n",
    "                 'difference_NN_GN', area_def_world, vmin=0, vmax=1,\n",
    "                                  proj=\"Orthographic\",var=\"u difference NN-Gaus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f92057",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reduced_lon_scene, reduced_lat_scene, reduced_data_scene =\\\n",
    "support_routines.get_Sat_frame(work_SAT_ds, area_def_world, chan=-1, \n",
    "              var = 'u10n_apriori_gauss_interp', begin_t=None, end_t=None)\n",
    "\n",
    "support_routines.basicMapPlotScat1(reduced_lon_scene, reduced_lat_scene, reduced_data_scene,\n",
    "                 'spaceTimeInterpolated', area_def_world, vmin=-25, vmax=25,\n",
    "                                  proj=\"Orthographic\",var=\"u component\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ac79a2",
   "metadata": {},
   "source": [
    "Sometimes we are interested in focusing on a specific region or location; we can do that by *zooming in* into the specific *area of interest*.\n",
    "We can create such area (afterwards interpretable using *Cartopy*) by defining a bounding box of *min/max* longitudes and latitudes. We select a *eqc* projection ([Plate Carree](https://proj.org/operations/projections/eqc.html) projection) as default. The datum defines the ellipsoid used in the transformations. \n",
    "\n",
    "I provide this auxiliary way of plotting specific regions, but it is by no means generic, its intended as a way to help visualize your data and it can definitely be improved!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a8bfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "corners = {\"min_lon\": 35 , \"max_lon\": 75, \"min_lat\": -30 , \"max_lat\": +30, \"lat_0\": 0, \"lon_0\":0}\n",
    "proj_id = 'eqc'  # eqc\n",
    "datum = 'WGS84'\n",
    "area_interest = support_routines.defineArea(corners, proj_id, datum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013461a7",
   "metadata": {},
   "source": [
    "Using the *area of interest* that we just defined we proced to get the *frame* (specific lon/lat/data combinations, 3 arrays) and we plot them using the *mapPlotScatZoom* function (in support/support_routines)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98d5931",
   "metadata": {},
   "outputs": [],
   "source": [
    "zoom_lon_scene, zoom_lat_scene, zoom_data_scene =\\\n",
    "support_routines.get_Sat_frame(work_SAT_ds, area_interest, chan=-1, \n",
    "              var = 'u10n_apriori_gauss_interp', begin_t=None, end_t=None)\n",
    "\n",
    "support_routines.mapPlotScatZoom(zoom_lon_scene, zoom_lat_scene, zoom_data_scene,\n",
    "                 'zoomPlot', -25,25,var='Apriori u10n', area=area_interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcadf311",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2acb18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa08b2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
