{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba0cbff4",
   "metadata": {},
   "source": [
    "# Spatial and temporal resampling of gridded ECMWF data onto a satellite's swath\n",
    "\n",
    "For the retrieval of physical parameters in the atmosphere via Optimal Estimation, we need apriori knowlegdge of the quantity we want to retrieve; this apriori knowledge can be obtained from a forecast model. \n",
    "\n",
    "ECMWF's forecasts are available in different spatial and temporal resolutions, in particular in this notebook we work with a *0.25/0.25* regular *lon/lat* grid in **space** and 16 steps for 2 [analysis](https://www.ecmwf.int/en/research/data-assimilation) times per day, time/step combination gives **temporal** coverage of data every hour: \n",
    "\n",
    "- *time* is the reference time (time at which the analysis is performed and observations are used to update the model).\n",
    "- *step* is related to the time steps that describe the temporal evolution of the forecast model.\n",
    "\n",
    "Once the ECMWF's data is available with a given spatial/temporal resolution, we focus on the satellite observations, to be specific the locations (i.e. longitude and latitude of each observation); because the satellite does not care much about grids, we rely on it's swath definition: how the instrument *samples* in space and time.\n",
    "\n",
    "Different instruments (and data providers) might have different versions of how to describe the swath (however the concept of swath is happily enough, unique); in our case we use CMSAF [data](https://wui.cmsaf.eu/safira/action/viewDoiDetails?acronym=FCDR_MWI_V003) (i.e. Brightness Temperatures). \n",
    "Each specific sample (observation) is made at a specific time and location (*time*, *lon*, *lat*), so our goal is: to interpolate the ECMWF's variable (which is defined on the regular *lon/lat* grid) onto the specific time/locations in our satellite's swath. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29ffd72",
   "metadata": {},
   "source": [
    "In this notebook we use: \n",
    "- [Pyresample](https://pyresample.readthedocs.io/en/latest/)'s functionality to efficiently resample data from a regular grid onto a swath.\n",
    "- [xarray](https://docs.xarray.dev/en/stable/)'s functionality to handle high dimensional datasets and to perform the time interpolation. \n",
    "\n",
    "First we will import the needed libraries and packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c342a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import xarray as xr\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "\n",
    "import cartopy\n",
    "import cartopy.crs as ccrs\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "\n",
    "import pyproj\n",
    "import pyresample\n",
    "from pyresample import create_area_def, load_area, data_reduce, utils, AreaDefinition\n",
    "from pyresample.geometry import SwathDefinition, GridDefinition\n",
    "from pyresample.kd_tree import resample_nearest, resample_gauss,\\\n",
    "                               get_neighbour_info, get_sample_from_neighbour_info\n",
    "from pyresample.bilinear import XArrayBilinearResampler, NumpyBilinearResampler #\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b0ccb7",
   "metadata": {},
   "source": [
    "I might use *Dask*'s functionality later on; when working with large datasets, *xarray* offers *Dask*'s capabilities to chunk, out-of-memory computation and parallel processing.\n",
    "\n",
    "Here we just create a computing cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebbc0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"MALLOC_TRIM_THRESHOLD_\"] = \"0\"#\"65536\"\n",
    "\n",
    "from dask.distributed import Client, progress, LocalCluster\n",
    "\n",
    "cluster = LocalCluster()\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3404236",
   "metadata": {},
   "source": [
    "We define some directories for easy access to the datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde466cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Satellite data:\n",
    "#dataSatDir = '/home/mario/Data/CMSAF/ssims/F16/'\n",
    "#dataSatDir = '/home/mario/Data/CMSAF/ssims/F16/ORD47662/'\n",
    "dataSatDir = '/home/mario/Data/CMSAF/ssims/F16/test_02Oct2014/'\n",
    "#dataSatDir = '/nobackup/users/echeverr/data/cmsaf/ssmis/F16/'\n",
    "#fileSatID = 'BTRin20140909000000324SSF1601GL.nc'\n",
    "\n",
    "# ECMWF data:\n",
    "#dataECMWFDir ='/home/mario/Data/Covariance_means/MARS_api_data/datasetsApriori/'\n",
    "#dataECMWFDir = '/nobackup/users/echeverr/data/ECMWF_era5/MARS_api_data/datasetsApriori/'\n",
    "dataECMWFDir ='/home/mario/Data/Covariance_means/MARS_api_data/datasetsAprioriRegGrid/'\n",
    "\n",
    "# REMOVE THE FOLLOWING ONCE YOU GET ECMWF'S DATASETS IN REGULAR GRID:\n",
    "#Test only:\n",
    "\n",
    "auxDataECMWFDir = '/home/mario/Data/Covariance_means/MARS_api_data/ERA5_data/datasets/'\n",
    "\n",
    "\n",
    "sys.path.append('support') # where supporting_routines_m live\n",
    "\n",
    "import support_routines "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d38d83",
   "metadata": {},
   "source": [
    "# Data preparation\n",
    "\n",
    "We have two sources of data that we care about in this notebook: ECMWF's data and satellite's swath definition (we do not use explicitely the observations per se, rather we use the location of the observations, the points where we want to resample and interpolate our ECMWF data)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da951760",
   "metadata": {},
   "source": [
    "First we load ECMWF's datasets using xarray; notice we open *profile* like datasets (i.e. with variables defined in pressure or model levels) and *surface* like datasets (i.e. with variables defined on the surface, e.g. 10m wind speed or 2m temperature).\n",
    "\n",
    "I then merge them into a single working dataset; shared dimensions and coordinates are automatically handled by xarray:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1b888b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "profile_info = xr.open_mfdataset(dataECMWFDir+'profiles*.grib', \n",
    "                                 engine=\"cfgrib\") #, chunks={'time': 50, 'latitude': 50, 'longitude': 200})\n",
    "surface_info = xr.open_mfdataset(dataECMWFDir+'surface*.grib', \n",
    "                                 engine=\"cfgrib\") #, chunks={'time': 50,'latitude': 50, 'longitude': 200})\n",
    "\n",
    "\n",
    "work_ds = profile_info.merge(surface_info).copy()\n",
    "\n",
    "work_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e5453f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block is only auxiliary: I used \"cdo\" to convert \n",
    "# a reduced Gaussian grid dataset (N320) into a \n",
    "# regular 0.25x0.25 deg**2 grid (ECMWF MARS was not\n",
    "# available at the time of this test, so I could not \n",
    "# download the dataset in the regular grid).\n",
    "# \"cdo\" uses a bilinear interpolation (cdo documentation); \n",
    "# but the resulting grid (lon, lat) has slight \n",
    "# differences respect to the ECMWF regular grid.\n",
    "\n",
    "# In this block I just take the (lon,lat) from another \n",
    "# 0.25x0.25 deg**2 ECMWF dataset and replace my cdo \n",
    "# regular grid with it for consistency.\n",
    "\n",
    "aux_info = xr.open_mfdataset(auxDataECMWFDir+'surface*.grib', \n",
    "                                 engine=\"cfgrib\")\n",
    "work_ds['latitude'] = aux_info['latitude'][::-1] # The order in cdo is different\n",
    "work_ds['longitude'] = aux_info['longitude']\n",
    "work_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7b9920",
   "metadata": {},
   "source": [
    "Having forecasts separated in *time*/*step* is not useful for our endeavour; we would like to have a single time reference to refer to.\n",
    "Also we want 24 hours coverage for all the period of interest, therefore we will stack each analysis time (there are 2 analysis per day in the operational forecasts, 00:00 and 12:00) with it's 12 steps.\n",
    "\n",
    "Because our end goal will be to use ECMWF's forecasts as apriori information in the Optimal Estimation (where we retrieve physical variables using observations), we want to avoid using data next to the analysis time (where observations are being [assimilated](https://www.ecmwf.int/en/research/data-assimilation)) in order to avoid undesired correlations between the apriori and the observations. It is standard to take a window of 3 hours after the analysis for the apriori data (you can notice this in the previous dataset, where the coordinate *step* starts at 03:00).  \n",
    "\n",
    "We use stack to have a single reference time at the end, we call this new reference time **time2** for simplicity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0070a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ECMWF_ds = work_ds.isel(step=slice(0,12)).stack(time2=(\"time\",\"step\"))\n",
    "ECMWF_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9979ba56",
   "metadata": {},
   "source": [
    "After stacking time/step the new multi-index variable **time2** is not very useful as a time reference, instead we create the new time dimension as the sum of the analysis time and the step (so time + step).\n",
    "\n",
    "It is useful to point out that *longitude* values for using *pyresample* *must* be refered to a *\\[-180, 180\\]* range (instead of *\\[0, 360\\]* )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0500b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ECMWF_ds['time2'] = (work_ds.isel(step=slice(0,12)).time + \n",
    " work_ds.isel(step=slice(0,12)).step).stack(time2=(\"time\",\"step\"))\n",
    "#ECMWF_ds['longitude'].values = ECMWF_ds.longitude.values - 180.0    # Reset lon to [-180,180]\n",
    "ECMWF_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c331711c",
   "metadata": {},
   "source": [
    "Our ECMWF's dataset seems to be ready to use; we now focus on our satellite observations.\n",
    "\n",
    "In this notebook we use CMSAF's [data](https://wui.cmsaf.eu/safira/action/viewDoiDetails?acronym=FCDR_MWI_V003) (Temperature Brightness).\n",
    "\n",
    "CMSAF's data is structured in logical groups, where each group coincides with a group of channels that share the same antenna of the instrument; this logical separation is very useful because different antennas will (likely) have effectively different footprints and sampling on the ground. \n",
    "\n",
    "We first open the datasets (7 days of observations, overlaping as much as possible in time with our ECMWF data); the open method in xarray (*open_mfdataset*) will open only the highest level in the datasets, this is useful to grasp the contents of the dataset (*channels*, *time*, *swath*, etc.). Notice that *open_mfdataset* at a difference with the more basic *open_dataset* will load the datasets in a *lazy* way using **Dask** [under the hood](https://docs.xarray.dev/en/stable/user-guide/dask.html) to avoid actually loading the data on memory:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730c88aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open satellite dataset at highest level (just to get the channels information):\n",
    "\n",
    "#ds = xr.open_dataset(dataSatDir+fileID)\n",
    "ds = xr.open_mfdataset(dataSatDir+'*.nc')\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66b8dda",
   "metadata": {},
   "source": [
    "Logical groups in NetCDF files (.nc) can be accessed directly with *xarray* if you know the name of the group (this can be easily accessed via the NetCDF4 library if not given by the data provider).\n",
    "\n",
    "For our particular application (10m wind speed retrieval via Optimal Estimation) we want to access the groups *scene_env1* and *scene_env2* (containing channels 19 and 37 GHz horizontal/vertical polarizations):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fee0a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open specific scenes containing the satellite observations:\n",
    "\n",
    "scenes_list = ['scene_env1', 'scene_env2']\n",
    "scene_BT = []\n",
    "\n",
    "for scene in scenes_list:        \n",
    "    scene_BT.append(xr.open_mfdataset(\n",
    "        dataSatDir+'*.nc', combine = 'nested', \n",
    "        concat_dim='time', group = scene)) \n",
    "\n",
    "#for scene in scenes_list:\n",
    "    #scene_BT.append(xr.open_dataset(dataSatDir+fileID, group = scene))\n",
    "    #scene_BT.append(xr.open_mfdataset(dataSatDir+'*.nc', group = scene))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb810d9",
   "metadata": {},
   "source": [
    "Lets check *scene_env1*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e28b5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_BT[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eab5eef",
   "metadata": {},
   "source": [
    "We can now  simply concatenate scenes *scene_env1* and *scene_env2*, given because they share the same swath definition (*lon*/*lat* values for each *time*/*scene_across_track* combination, you can check this in CMSAF's product [documentation](https://www.cmsaf.eu/SharedDocs/Literatur/document/2016/saf_cm_dwd_pum_fcdr_ssmis_1_4_pdf.pdf?__blob=publicationFile))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dad8276",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_BT = xr.concat(scene_BT, dim = 'scene_channel') #.drop_vars([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a85f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_BT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45581dc",
   "metadata": {},
   "source": [
    "After concatenating through a dimension (in our case through *scene_channel*) *xarray* fill's in any \"missing\" information; in our case we se that *xarray* added the dimension *scene_channel* to variables that do not really depend on it.\n",
    "\n",
    "Here we simply correct this by selecting the first element in this dimension for all the variables that actually do not depend on it; we also notice that the attributes (e.b. comment: channels h19, v19, etc.) are not complete after the concatenation, **we will not focus on this** in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db80522a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_BT['lat'] = ds_BT.lat[0,:,:]\n",
    "ds_BT['lon'] = ds_BT.lon[0,:,:]\n",
    "ds_BT['eia'] = ds_BT.eia[0,:,:]\n",
    "ds_BT['sft'] = ds_BT.sft[0,:,:]\n",
    "ds_BT['qc_fov'] = ds_BT.qc_fov[0,:,:]\n",
    "ds_BT['laz'] = ds_BT.laz[0,:,:]\n",
    "\n",
    "# And visualize again:\n",
    "ds_BT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32a2111",
   "metadata": {},
   "source": [
    "Finally we want to keep track of the frequency of the channels that we will use, so we select the channels that we will use (and only over the ocean in this case, where sft==0). We also copy the *central_freq* and *polarization* variables and use the values that we already had in our higher level dataset *ds*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3dee53",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_aux = ds_BT.assign_coords(time=ds.time).sel(\n",
    "    scene_channel=[11,12,14,15]).where(ds_BT.sft==0)\n",
    "\n",
    "# if opening 1 day of observations:\n",
    "ds_aux['central_freq'] = ds['central_freq'][ds_aux['scene_channel']]\n",
    "ds_aux['polarization'] = ds['polarization'][ds_aux['scene_channel']]\n",
    "\n",
    "# if opening more observations files:\n",
    "#ds_aux['central_freq'] = ds['central_freq'][0,0,ds_aux['scene_channel']]\n",
    "#ds_aux['polarization'] = ds['polarization'][0,0,ds_aux['scene_channel']]\n",
    "\n",
    "\n",
    "# Create working satellite dataset (setting 'scene_channel' as last dimension):\n",
    "\n",
    "SAT_ds = ds_aux.transpose(...,'scene_channel') #.drop_dims(drop_dims = ['date','channel'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce27c8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAT_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16852296",
   "metadata": {},
   "source": [
    "## Spatial and temporal resampling:\n",
    "We now obtain the apriori data that will be used during the optimal estimation retrievals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730eb6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def computeApriori(ECMWF_ds, work_SAT_ds, variables, overlapTime):\n",
    "    \n",
    "    # Find initial and final times of the batch:\n",
    "    init_time = np.min(work_SAT_ds.time.values)\n",
    "    end_time = np.max(work_SAT_ds.time.values)\n",
    "    \n",
    "    # For time interpolation:\n",
    "    # We want to interpolate to the middle of the observation batch:\n",
    "    time_interp = init_time + (end_time-init_time)/2 \n",
    "\n",
    "    # Select the nearest valid time i.e. a time instant\n",
    "    # that exists in the observations:\n",
    "    time_interp = work_SAT_ds.time.sel(time=time_interp, method = \"nearest\")    \n",
    "    \n",
    "    # Select time slices from ECMWF data:\n",
    "    delta_h = np.timedelta64(overlapTime, 'h') # Useful delta to create overlap in time\n",
    "\n",
    "    timeOverlapInit = ECMWF_ds.time2.sel(\n",
    "        time2 = work_SAT_ds.time.min() - delta_h, method = \"nearest\")\n",
    "\n",
    "    timeOverlapEnd = ECMWF_ds.time2.sel(\n",
    "        time2=work_SAT_ds.time.max() + delta_h, method = \"nearest\")\n",
    "\n",
    "    work_ECMWF_ds = ECMWF_ds.sel(time2 = \n",
    "                             slice(timeOverlapInit,timeOverlapEnd)\n",
    "                            )\n",
    "\n",
    "    work_ECMWF_ds = work_ECMWF_ds.transpose(...,\n",
    "                                        'latitude','longitude','time2')\n",
    "\n",
    "    # Define swath using PyResample's SwathDefinition (geometry def.): \n",
    "    SAT_swath_def = SwathDefinition(lons = work_SAT_ds.lon.values, \n",
    "                                lats = work_SAT_ds.lat.values)\n",
    "\n",
    "    # Define grid using PyResample's GridDefiniton (geometry def.):\n",
    "    lon2d,lat2d = np.meshgrid(work_ECMWF_ds.longitude, \n",
    "                          work_ECMWF_ds.latitude)\n",
    "    ECMWF_grid_def = GridDefinition(lons=lon2d, lats=lat2d)\n",
    "    \n",
    "    return resampleInterpolate(work_ECMWF_ds, ECMWF_grid_def, \n",
    "                               SAT_swath_def, time_interp, \n",
    "                               variables, work_SAT_ds, \n",
    "                        radius_of_influence = 30000,\n",
    "                        resample_type = 'custom',  # 'nn' or 'custom' for gaussian or custom weights\n",
    "                        neighbours=10,\n",
    "                        weight_type = 'gauss',\n",
    "                        sigmas =\\\n",
    "                               30000*np.ones(\n",
    "                            len(work_ECMWF_ds.time2.values))\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1851d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# From: https://github.com/pytroll/pyresample/blob/main/pyresample/kd_tree.py\n",
    "import types\n",
    "if sys.version >= '3':\n",
    "    long = int\n",
    "def gauss(sigma):\n",
    "        # Return gauss function object\n",
    "        return lambda r: np.exp(-r ** 2 / float(sigma) ** 2)\n",
    "# End from.\n",
    "\n",
    "\n",
    "def resampleInterpolate(work_ECMWF_ds, ECMWF_grid_def,\n",
    "                        SAT_swath_def, time_interp, \n",
    "                        variables, work_SAT_ds, \n",
    "                        radius_of_influence = 30000,\n",
    "                        resample_type = 'nn',\n",
    "                        neighbours=1,\n",
    "                        weight_type = 'gauss',\n",
    "                       sigmas = None):\n",
    "        \n",
    "    # Spatial resampling (need to 'loop' in levels if profile-like variables):\n",
    "    # Not implemented.\n",
    "    \n",
    "    # For resampling multiple variables in a dataset, \n",
    "    # it is more efficient to obtain the neighbours\n",
    "    # information once (this depends only on the \n",
    "    # geometry information):\n",
    "    #startTime = time.time()\n",
    "    valid_input_index, valid_output_index, index_array, distance_array = \\\n",
    "                               get_neighbour_info(ECMWF_grid_def,\n",
    "                                                  SAT_swath_def, \n",
    "                                                  radius_of_influence = radius_of_influence,\n",
    "                                                  neighbours = neighbours)\n",
    "    #print(\"%.2f s , End get_neighbour\" % (time.time()-startTime)) \n",
    "    \n",
    "    if(resample_type == 'nn'):\n",
    "        weight_funcs = None\n",
    "    elif(resample_type == 'custom'):\n",
    "        if(weight_type == 'gauss'):\n",
    "            if(sigmas.any() == None):\n",
    "                sigmas = radius_of_influence*np.ones(\n",
    "                    len(work_ECMWF_ds.time2.values))\n",
    "                print('Sigmas not provided for Gaussian Nearest Neighbour resampling.')\n",
    "                print('Using sigmas = radius_of_influence as default.')\n",
    "                \n",
    "            # From: https://github.com/pytroll/pyresample/blob/main/pyresample/kd_tree.py  ********\n",
    "            is_multi_channel = False\n",
    "            try:\n",
    "                sigmas.__iter__()\n",
    "                sigma_list = sigmas\n",
    "                is_multi_channel = True\n",
    "            except AttributeError:\n",
    "                   sigma_list = [sigmas]\n",
    "\n",
    "            for sigma in sigma_list:\n",
    "                if not isinstance(sigma, (long, int, float)):\n",
    "                    raise TypeError('sigma must be number')\n",
    "\n",
    "            # Get gauss function objects\n",
    "            if is_multi_channel:\n",
    "                weight_funcs = list(map(gauss, sigma_list))\n",
    "            else:\n",
    "                weight_funcs = gauss(sigmas)\n",
    "            # End from.   ****************************\n",
    "            \n",
    "            \n",
    "            #elif(other_type_of_radial_weight? Go ahead and define them)\n",
    "        else:\n",
    "            print('Weight type not supported; resampling with Nearest Neighbour as default')\n",
    "            resample_type == 'nn'\n",
    "            weight_funcs = None\n",
    "\n",
    "       \n",
    "    for variable in variables:\n",
    "        \n",
    "        print('Variable: '+variable)\n",
    "        \n",
    "        #startTime = time.time()\n",
    "        #dataAux = get_sample_from_neighbour_info(resample_type, SAT_swath_def.shape, \n",
    "        #                                             work_ECMWF_ds[variable].values,\n",
    "        #                                     valid_input_index, valid_output_index,\n",
    "        #                                     index_array, distance_array = distance_array,\n",
    "        #                                         weight_funcs = weight_funcs )\n",
    "        #print(\"%.2f s , End get_sample\" % (time.time()-startTime))\n",
    "        \n",
    "        spatialResampling = xr.DataArray(\n",
    "                \n",
    "                data = get_sample_from_neighbour_info(resample_type, SAT_swath_def.shape, \n",
    "                                                     work_ECMWF_ds[variable].values,\n",
    "                                             valid_input_index, valid_output_index,\n",
    "                                             index_array, distance_array = distance_array,\n",
    "                                                 weight_funcs = weight_funcs ),\n",
    "        \n",
    "                #data   = resample_gauss(ECMWF_grid_def, work_ECMWF_ds[variable].values, \\\n",
    "               #SAT_swath_def, radius_of_influence=30000, neighbours=10,\\\n",
    "               #sigmas=30000*np.ones(len(work_ECMWF_ds.time2.values)), fill_value=None),  # enter data here\n",
    "\n",
    "                dims   = ['time','scene_across_track','time4interpolation'],\n",
    "                coords = {'time': work_SAT_ds.time, \n",
    "                          'scene_across_track': work_SAT_ds.scene_across_track,\n",
    "                         'time4interpolation': work_ECMWF_ds.time2.values},\n",
    "                attrs  = {\n",
    "                    #'_FillValue': -999.9,\n",
    "                    'description': variable+' from ECMWFs forecast resampled with\\\n",
    "                    PyResample (Nearest Neighbour resampler) to satellite swath',\n",
    "                    'units'     : 'm/s'\n",
    "                    }\n",
    "                ) #.chunk({\"time\": chunk_size_time,\n",
    "                  #       \"scene_across_track\": chunk_size_s_a_t})\n",
    "        \n",
    "        # Interpolate using xarray's (scipy under the hood) capabilities:\n",
    "        # xarray.interp documentation: \n",
    "        # https://docs.xarray.dev/en/stable/user-guide/interpolation.html\n",
    "    \n",
    "        if(len(work_ECMWF_ds.time2.values)<4): # Cubic interpolation requires at least 4 points\n",
    "        # Linear interpolation:\n",
    "            work_SAT_ds[variable+'_apriori'] =\\\n",
    "               spatialResampling.where(work_SAT_ds.sft==0).interp(\n",
    "                time4interpolation = time_interp.values, method=\"linear\")    \n",
    "        else:\n",
    "        # Higher order interpolation:\n",
    "            work_SAT_ds[variable+'_apriori'] =\\\n",
    "                spatialResampling.where(work_SAT_ds.sft==0).interpolate_na(\"time\").dropna(\"time\")\\\n",
    "                   .interp(time4interpolation=time_interp.values, method=\"cubic\")\n",
    "    \n",
    "        work_SAT_ds[variable+'_apriori'].attrs = {\n",
    "                    #'_FillValue': -999.9,\n",
    "                    'description': variable+' from ECMWFs forecast spatio-temporally resampled and interpolated',\n",
    "                    'units'     : 'm/s'\n",
    "                    }\n",
    "    #.chunk({\"time\": None,\"scene_across_track\": None})\n",
    "    return work_SAT_ds     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cac1319",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "chunk_size_time = 60\n",
    "\n",
    "# We want obtain apriori data for a specific\n",
    "# set of observations (e.g. 1 day of data).\n",
    "# Assuming for example 1 day of data (24 hours),\n",
    "# we will work with a batch size of 1 hour (for starters\n",
    "# this can be changed and tests carried out\n",
    "# for different batch sizes).\n",
    "\n",
    "SAT_ds_grouped = SAT_ds.resample(time='1H') # '0.3H'\n",
    "\n",
    "overlapTime = 1\n",
    "variables = ['v10n', 'u10n', 'skt']\n",
    "\n",
    "\n",
    "startTime = time.time()\n",
    "\n",
    "i = 0\n",
    "for hour_name, hour_group in SAT_ds_grouped:\n",
    "    \n",
    "    hour_group = computeApriori(ECMWF_ds, hour_group, \n",
    "                                 variables, overlapTime) \n",
    "    if(i==0):\n",
    "        apriori_da = hour_group #[variable+'_apriori'] \n",
    "    else:\n",
    "        apriori_da = xr.concat(( apriori_da,\n",
    "                                hour_group ), dim = \"time\") #[variable+'_apriori']), dim = \"time\")\n",
    "    i+=1\n",
    "    \n",
    "\n",
    "SAT_ds = SAT_ds.merge(apriori_da)\n",
    "\n",
    "print(\"%.2f s , Time_Hour\" % (time.time()-startTime))  \n",
    "\n",
    "\n",
    "#.chunk({\"time\": None,\"scene_across_track\": None})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a85e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "apriori_da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c81383a",
   "metadata": {},
   "outputs": [],
   "source": [
    "area_def_world = load_area('areas.yaml', 'worldeqc30km')# 'worldeqc30km70') # for plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d07c152",
   "metadata": {},
   "outputs": [],
   "source": [
    "initSat_time = np.datetime64('2014-10-02T00:00:00.000') \n",
    "endSat_time = np.datetime64('2014-10-02T02:00:00.000')\n",
    "\n",
    "init_time = apriori_da.time.sel(time=initSat_time, method = \"nearest\")\n",
    "end_time = apriori_da.time.sel(time=endSat_time, method = \"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edae89ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for variable in variables:\n",
    "    reduced_lon_scene, reduced_lat_scene, reduced_data_scene =\\\n",
    "    support_routines.get_Sat_frame(\\\n",
    "                               SAT_ds.sel(\n",
    "        time=slice(init_time,end_time)), area_def_world, chan=-1, \n",
    "                               var = variable+'_apriori', \n",
    "                               begin_t=None, end_t=None)\n",
    "\n",
    "    support_routines.basicMapPlotScat1(reduced_lon_scene, reduced_lat_scene, \n",
    "                                       reduced_data_scene,\n",
    "                                   variable+'test_times', area_def_world, \n",
    "                                       vmin=-25, vmax=25,\n",
    "                              proj=\"PlateCarree\",var=variable+'_apriori')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af1b834",
   "metadata": {},
   "source": [
    "At this point we have our ECMWF data resampled/interpolated in space/time and so the data is ready to use; lets visualize a few results.\n",
    "\n",
    "First, lets create an area of interest to plot, in this case I want to plot some of the computed variables on the entire globe:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868efaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_lons_interest, grid_lats_interest = area_def_world.get_lonlats()\n",
    "ds = SAT_ds.merge(apriori_da).sel(\n",
    "    time=slice(init_time,end_time))\n",
    "swathDef = SwathDefinition(lons=ds.lon.to_numpy(), lats=ds.lat.to_numpy())\n",
    "lon_scene, lat_scene = swathDef.get_lonlats()\n",
    "\n",
    "reduced_lon_scene, reduced_lat_scene, reduced_data_scene = \\\n",
    "                           data_reduce.swath_from_lonlat_grid(\n",
    "            grid_lons_interest, grid_lats_interest,\n",
    "            lon_scene, lat_scene, ds[variable+'_apriori'][:,:].compute().to_numpy(),\n",
    "            radius_of_influence=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f660eb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[variable+'_apriori'].chunk({\"time\": None,\"scene_across_track\": None}).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35a3e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "area_def_world = load_area('areas.yaml', 'worldeqc30km')# 'worldeqc30km70') # for plots\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93654577",
   "metadata": {},
   "source": [
    "Lets visualize our observations (remember we selected a subset or **batch**):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ed6b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for channel in range(4):\n",
    "    reduced_lon_scene, reduced_lat_scene, reduced_data_scene =\\\n",
    "    support_routines.get_Sat_frame(work_SAT_ds, area_def_world, chan=channel, \n",
    "              var = 'tb', begin_t=None, end_t=None)    \n",
    "    support_routines.basicMapPlotScat1(reduced_lon_scene, reduced_lat_scene, reduced_data_scene,\n",
    "                 'scene_channel_'+str(channel), area_def_world, vmin=130, vmax=270,\n",
    "                                      proj=\"Orthographic\", \n",
    "                                       var = 'Channel: '+str(work_SAT_ds.central_freq[channel].values)+' '+\n",
    "                                       str(work_SAT_ds.central_freq[channel].units)+' '+\n",
    "                                      str(work_SAT_ds.polarization[channel].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbe46c9",
   "metadata": {},
   "source": [
    "For simplicity in the plotting I have created a single routine (**basicMapPlotScat1**) for plotting *swath* data (i.e. non regular grid data), we can visualize observations but also the resampled (and in the end, interpolated) variables; argument *chan* in the plotting routine will tell the routine wheter I want to visualize observations (*swath* + *channel*), resampled variables (*swath* + *time4interpolation*) or resampled **and** time interpolated variables (*swath* only).\n",
    "\n",
    "Now lets check the spatially resampled *u* component of the wind speed (i.e. not interpolated in time yet); for this I select one of the time slices that I resampled (*time4interpolation*):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2ce86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the resampled (Nearest neighb.) ECMWF data in the new 'grid' \n",
    "# (i.e. the satellite swath):\n",
    "\n",
    "reduced_lon_scene, reduced_lat_scene, reduced_data_scene =\\\n",
    "support_routines.get_Sat_frame(work_SAT_ds, area_def_world, chan=3, \n",
    "              var = variable+'_spatial', begin_t=None, end_t=None)\n",
    "\n",
    "support_routines.basicMapPlotScat1(reduced_lon_scene, reduced_lat_scene, reduced_data_scene,\n",
    "                 'resampledNN', area_def_world, vmin=-25, vmax=25, \n",
    "                                   proj=\"Orthographic\", var = variable+'_spatial')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d98ce5b",
   "metadata": {},
   "source": [
    "It seems to produce a good (eye ball metric) resampled variable; lets check the ECMWF's gridded data on the same time slice: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2227de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot origin data (ECMWF on regular grid, to compare with the resampled one):\n",
    "\n",
    "fig = plt.figure()\n",
    "#ax = plt.axes(projection=ccrs.PlateCarree())  #\n",
    "ax = plt.axes(projection=ccrs.Orthographic(60,-15))\n",
    "ax.add_feature(cartopy.feature.LAND, zorder=0, edgecolor='black')\n",
    "ax.coastlines()\n",
    "ax.gridlines()\n",
    "work_ECMWF_ds[variable][:,:,3].where(\n",
    "    work_ECMWF_ds.lsm[:,:,3]==0).plot(ax=ax,\n",
    "    transform=ccrs.PlateCarree(), cmap=\"viridis\",\n",
    "                                     vmin=-25, vmax=25)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "#plt.savefig('allWind_and_swath.png', bbox_inches='tight', dpi=300) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee949483",
   "metadata": {},
   "source": [
    "Now lets visualize the difference between the *Gaussian Nearest* and *Nearest Neighbours* methods in the **final resampled/interpolated (i.e. space/time)** variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f92057",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reduced_lon_scene, reduced_lat_scene, reduced_data_scene =\\\n",
    "support_routines.get_Sat_frame(work_SAT_ds, area_def_world, chan=-1, \n",
    "              var = variable+'_apriori', begin_t=None, end_t=None)\n",
    "\n",
    "support_routines.basicMapPlotScat1(reduced_lon_scene, reduced_lat_scene, reduced_data_scene,\n",
    "                 'spaceTimeInterpolated', area_def_world, vmin=-25, vmax=25,\n",
    "                                  proj=\"Orthographic\",var=variable+'_apriori')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37e47c5",
   "metadata": {},
   "source": [
    "Sometimes we are interested in focusing on a specific region or location; we can do that by *zooming in* into the specific *area of interest*.\n",
    "We can create such area (afterwards interpretable using *Cartopy*) by defining a bounding box of *min/max* longitudes and latitudes. We select a *eqc* projection ([Plate Carree](https://proj.org/operations/projections/eqc.html) projection) as default. The datum defines the ellipsoid used in the transformations. \n",
    "\n",
    "I provide this auxiliary way of plotting specific regions, but it is by no means generic, its intended as a way to help visualize your data and it can definitely be improved!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a8bfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "corners = {\"min_lon\": 35 , \"max_lon\": 75, \"min_lat\": -30 , \"max_lat\": +30, \"lat_0\": 0, \"lon_0\":0}\n",
    "proj_id = 'eqc'  # eqc\n",
    "datum = 'WGS84'\n",
    "area_interest = support_routines.defineArea(corners, proj_id, datum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c286c282",
   "metadata": {},
   "source": [
    "Using the *area of interest* that we just defined we proced to get the *frame* (specific lon/lat/data combinations, 3 arrays) and we plot them using the *mapPlotScatZoom* function (in support/support_routines)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98d5931",
   "metadata": {},
   "outputs": [],
   "source": [
    "zoom_lon_scene, zoom_lat_scene, zoom_data_scene =\\\n",
    "support_routines.get_Sat_frame(work_SAT_ds, area_interest, chan=-1, \n",
    "              var = variable+'_apriori', begin_t=None, end_t=None)\n",
    "\n",
    "support_routines.mapPlotScatZoom(zoom_lon_scene, zoom_lat_scene, zoom_data_scene,\n",
    "                 variable+'_zoomPlot', -25,25,var=variable+'_apriori', area=area_interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcadf311",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2acb18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa08b2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
